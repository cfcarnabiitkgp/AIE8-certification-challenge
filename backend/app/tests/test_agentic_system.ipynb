{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Paper Peer Review System - Agentic Testing\n",
    "\n",
    "This notebook tests the LangGraph-based agentic system with structured outputs using Pydantic models.\n",
    "\n",
    "## Features Tested:\n",
    "- LangGraph StateGraph workflow\n",
    "- Pydantic BaseModel state management\n",
    "- Structured outputs from all agents\n",
    "- ClarityAgent with ReAct pattern\n",
    "- RigorAgent with ReAct pattern\n",
    "- Orchestrator validation with structured decision making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend path: /Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend\n",
      "OpenAI API Key loaded: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add backend to path (we're in app/tests, so go up 2 levels to backend root)\n",
    "backend_path = Path(__file__).parent.parent.parent if '__file__' in globals() else Path.cwd().parent.parent\n",
    "if str(backend_path) not in sys.path:\n",
    "    sys.path.insert(0, str(backend_path))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(f\"Backend path: {backend_path}\")\n",
    "print(f\"OpenAI API Key loaded: {'OPENAI_API_KEY' in os.environ}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'app.agents.section.section_analyzer' from '/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/app/agents/section/section_analyzer.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import app.agents.section.section_analyzer\n",
    "importlib.reload(app.agents.section.section_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Import the agentic system components\n",
    "from app.agents.review_controller_langgraph import LangGraphReviewController\n",
    "from app.agents.clarity import ClarityAgent\n",
    "from app.agents.rigor import RigorAgent\n",
    "from app.agents.section import SectionAnalyzer\n",
    "from app.models.schemas import (\n",
    "    ReviewState,\n",
    "    Section,\n",
    "    Suggestion,\n",
    "    ClarityIssue,\n",
    "    ClarityAnalysisResponse,\n",
    "    RigorIssue,\n",
    "    OrchestratorDecision\n",
    ")\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Research Paper\n",
    "\n",
    "We'll use a sample paper with various issues to test the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample paper length: 3075 characters\n",
      "Sample paper word count: ~430 words\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_PAPER = \"\"\"\n",
    "# Neural Architecture Search with Reinforcement Learning\n",
    "\n",
    "# 1. Abstract\n",
    "\n",
    "We present a novel approach to NAS using RL. Our method achieves SOTA results on ImageNet.\n",
    "The architecture it discovered outperformed manually designed architectures by a significant margin.\n",
    "\n",
    "# 2. Introduction\n",
    "\n",
    "Deep learning has revolutionized computer vision. However, designing neural architectures remains \n",
    "a time-consuming process that requires expert knowledge. This is problematic because different \n",
    "tasks may require different architectures, and it's not always clear what architecture will work \n",
    "best for a given task.\n",
    "\n",
    "In this work, we propose using RL to automatically search for optimal neural architectures. \n",
    "Our approach uses a controller RNN to generate architecture descriptions, which are then trained \n",
    "and evaluated. The validation accuracy is used as the reward signal to train the controller.\n",
    "\n",
    "# 3. Methods\n",
    "\n",
    "This section describes our neural architecture search methodology, including the search space \n",
    "design and training procedures.\n",
    "\n",
    "## 3.1 Architecture Search Space\n",
    "\n",
    "Our search space includes various layer types including convolutional layers, pooling layers, \n",
    "and skip connections. The controller generates a sequence of tokens that specifies the architecture.\n",
    "Each token represents a decision about the architecture like layer type, kernel size, etc.\n",
    "\n",
    "## 3.2 Training Procedure\n",
    "\n",
    "We trained the controller using REINFORCE. The controller generates 100 architectures per iteration.\n",
    "Each architecture is trained for 50 epochs on CIFAR-10. We used the validation accuracy as the \n",
    "reward signal. The controller is then updated using policy gradients.\n",
    "\n",
    "### 3.2.1 Hyperparameters\n",
    "\n",
    "The learning rate was set to 0.001 with Adam optimizer. We used a batch size of 32 for training\n",
    "the controller network.\n",
    "\n",
    "# 4. Results\n",
    "\n",
    "Our method discovered architectures that achieved 96.4% accuracy on CIFAR-10 test set. This \n",
    "outperformed the previous best result. The discovered architecture also transferred well to \n",
    "ImageNet, achieving competitive accuracy.\n",
    "\n",
    "Figure 1 shows the accuracy over time. As you can see, it improves significantly. The final \n",
    "architecture found by our method has an interesting structure with multiple skip connections.\n",
    "\n",
    "## 4.1 Comparison with Baselines\n",
    "\n",
    "We compared our approach against manually designed architectures and other NAS methods. Our \n",
    "method achieved superior performance while requiring less computational resources.\n",
    "\n",
    "# 5. Discussion\n",
    "\n",
    "The results demonstrate that RL can effectively search for neural architectures. One limitation \n",
    "is the computational cost - searching for architectures required significant GPU resources. \n",
    "Future work could explore more efficient search methods.\n",
    "\n",
    "Our approach has implications for AutoML and could make deep learning more accessible. It shows \n",
    "that automated methods can match or exceed human-designed architectures.\n",
    "\n",
    "# 6. Conclusion\n",
    "\n",
    "We presented a method for neural architecture search using reinforcement learning. The approach \n",
    "successfully discovered high-performing architectures that outperformed manual designs.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Sample paper length: {len(SAMPLE_PAPER)} characters\")\n",
    "print(f\"Sample paper word count: ~{len(SAMPLE_PAPER.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Section Analyzer\n",
    "\n",
    "Test the markdown parser that converts content into Section Pydantic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Total sections parsed: 6\n",
      "\n",
      "1. Abstract\n",
      "   Level: 1\n",
      "   Lines: 4-8\n",
      "   Content length: 194 chars\n",
      "   Type: Section\n",
      "\n",
      "2. Introduction\n",
      "   Level: 1\n",
      "   Lines: 9-19\n",
      "   Content length: 603 chars\n",
      "   Type: Section\n",
      "\n",
      "3. Methods\n",
      "   Level: 1\n",
      "   Lines: 20-41\n",
      "   Content length: 902 chars\n",
      "   Type: Section\n",
      "\n",
      "4. Results\n",
      "   Level: 1\n",
      "   Lines: 42-55\n",
      "   Content length: 632 chars\n",
      "   Type: Section\n",
      "\n",
      "5. Discussion\n",
      "   Level: 1\n",
      "   Lines: 56-64\n",
      "   Content length: 420 chars\n",
      "   Type: Section\n",
      "\n",
      "6. Conclusion\n",
      "   Level: 1\n",
      "   Lines: 65-69\n",
      "   Content length: 187 chars\n",
      "   Type: Section\n",
      "\n",
      "✓ All sections are valid Pydantic Section models\n"
     ]
    }
   ],
   "source": [
    "analyzer = SectionAnalyzer()\n",
    "sections = analyzer.parse_markdown(SAMPLE_PAPER)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SECTION ANALYSIS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Total sections parsed: {len(sections)}\\n\")\n",
    "\n",
    "for i, section in enumerate(sections, 1):\n",
    "    print(f\"{i}. {section.title}\")\n",
    "    print(f\"   Level: {section.level}\")\n",
    "    print(f\"   Lines: {section.line_start}-{section.line_end}\")\n",
    "    print(f\"   Content length: {len(section.content)} chars\")\n",
    "    print(f\"   Type: {type(section).__name__}\")\n",
    "    print()\n",
    "\n",
    "# Verify it's a Pydantic model\n",
    "assert isinstance(sections[0], Section), \"Sections should be Pydantic Section models\"\n",
    "print(\"✓ All sections are valid Pydantic Section models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Section(title='Abstract', content='\\nWe present a novel approach to NAS using RL. Our method achieves SOTA results on ImageNet.\\nThe architecture it discovered outperformed manually designed architectures by a significant margin.\\n\\n', level=1, line_start=4, line_end=8, section_number='1', parent_section=None, subsections=[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Individual Agent Testing\n",
    "\n",
    "Test ClarityAgent and RigorAgent with structured outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Agents initialized successfully\n",
      "  - ClarityAgent: ClarityAgent\n",
      "  - RigorAgent: RigorAgent\n"
     ]
    }
   ],
   "source": [
    "# Initialize agents\n",
    "clarity_agent = ClarityAgent()\n",
    "rigor_agent = RigorAgent()\n",
    "\n",
    "print(\"✓ Agents initialized successfully\")\n",
    "print(f\"  - ClarityAgent: {clarity_agent.agent_name}\")\n",
    "print(f\"  - RigorAgent: {rigor_agent.agent_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test ClarityAgent on Abstract Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTING CLARITY AGENT\n",
      "================================================================================\n",
      "\n",
      "Section: Abstract\n",
      "Content:\n",
      "\n",
      "We present a novel approach to NAS using RL. Our method achieves SOTA results on ImageNet.\n",
      "The architecture it discovered outperformed manually designed architectures by a significant margin.\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CLARITY ANALYSIS COMPLETE (12.49s)\n",
      "================================================================================\n",
      "\n",
      "Found 3 clarity issues:\n",
      "\n",
      "1. Clarity Issue\n",
      "   Severity: warning\n",
      "   Issue: The term 'NAS' is not defined, which may confuse readers unfamiliar with the acronym....\n",
      "   Fix: Define 'NAS' as 'Neural Architecture Search' in the first sentence....\n",
      "\n",
      "2. Clarity Issue\n",
      "   Severity: warning\n",
      "   Issue: The phrase 'SOTA results' may not be clear to all readers....\n",
      "   Fix: Replace 'SOTA results' with 'state-of-the-art results' for clarity....\n",
      "\n",
      "3. Clarity Issue\n",
      "   Severity: error\n",
      "   Issue: The phrase 'by a significant margin' is vague and does not provide quantitative context....\n",
      "   Fix: Provide specific metrics or percentages to quantify the performance improvement over manually design...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find abstract section\n",
    "abstract_section = next((s for s in sections if 'abstract' in s.title.lower()), None)\n",
    "\n",
    "if abstract_section:\n",
    "    # Convert to dict for agent (agents expect dict input)\n",
    "    section_dict = {\n",
    "        \"title\": abstract_section.title,\n",
    "        \"content\": abstract_section.content,\n",
    "        \"level\": abstract_section.level,\n",
    "        \"line_start\": abstract_section.line_start,\n",
    "        \"line_end\": abstract_section.line_end\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TESTING CLARITY AGENT\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"Section: {abstract_section.title}\")\n",
    "    print(f\"Content:\\n{abstract_section.content[:200]}...\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    clarity_suggestions = await clarity_agent.analyze(section_dict)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CLARITY ANALYSIS COMPLETE ({elapsed:.2f}s)\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"Found {len(clarity_suggestions)} clarity issues:\\n\")\n",
    "    \n",
    "    for i, suggestion in enumerate(clarity_suggestions, 1):\n",
    "        print(f\"{i}. {suggestion['title']}\")\n",
    "        print(f\"   Severity: {suggestion['severity']}\")\n",
    "        print(f\"   Issue: {suggestion['description'][:100]}...\")\n",
    "        print(f\"   Fix: {suggestion['suggested_fix'][:100]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No abstract section found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test RigorAgent on Methods Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTING RIGOR AGENT\n",
      "================================================================================\n",
      "\n",
      "Section: Methods\n",
      "Content:\n",
      "\n",
      "This section describes our neural architecture search methodology, including the search space \n",
      "design and training procedures.\n",
      "\n",
      "\n",
      "## 3.1. Architecture Search Space\n",
      "\n",
      "\n",
      "Our search space includes various ...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RIGOR ANALYSIS COMPLETE (12.87s)\n",
      "================================================================================\n",
      "\n",
      "Found 3 rigor issues:\n",
      "\n",
      "1. Rigor Issue\n",
      "   Severity: warning\n",
      "   Issue: Lack of detail on search space design...\n",
      "   Fix: Provide a more detailed description of the specific layer types and their configurations included in...\n",
      "\n",
      "2. Rigor Issue\n",
      "   Severity: warning\n",
      "   Issue: Insufficient detail on training procedure...\n",
      "   Fix: Include more information on the training process, such as the number of iterations for training the ...\n",
      "\n",
      "3. Rigor Issue\n",
      "   Severity: warning\n",
      "   Issue: Missing justification for hyperparameter choices...\n",
      "   Fix: Add a rationale for the choice of learning rate, optimizer, and batch size, including any experiment...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find methods section\n",
    "methods_section = next((s for s in sections if 'method' in s.title.lower()), None)\n",
    "\n",
    "if methods_section:\n",
    "    section_dict = {\n",
    "        \"title\": methods_section.title,\n",
    "        \"content\": methods_section.content,\n",
    "        \"level\": methods_section.level,\n",
    "        \"line_start\": methods_section.line_start,\n",
    "        \"line_end\": methods_section.line_end\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TESTING RIGOR AGENT\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"Section: {methods_section.title}\")\n",
    "    print(f\"Content:\\n{methods_section.content[:200]}...\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    rigor_suggestions = await rigor_agent.analyze(section_dict)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RIGOR ANALYSIS COMPLETE ({elapsed:.2f}s)\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(f\"Found {len(rigor_suggestions)} rigor issues:\\n\")\n",
    "    \n",
    "    for i, suggestion in enumerate(rigor_suggestions, 1):\n",
    "        print(f\"{i}. {suggestion['title']}\")\n",
    "        print(f\"   Severity: {suggestion['severity']}\")\n",
    "        print(f\"   Issue: {suggestion['description'][:100]}...\")\n",
    "        print(f\"   Fix: {suggestion['suggested_fix'][:100]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No methods section found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Full LangGraph Workflow\n",
    "\n",
    "Test the complete LangGraph StateGraph with all agents and orchestrator validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LangGraph ReviewController initialized\n",
      "  Workflow nodes: parse_sections, analyze_section, next_section, validate_suggestions\n",
      "  Conditional edges: continue/validate based on section index\n"
     ]
    }
   ],
   "source": [
    "# Initialize LangGraph controller\n",
    "controller = LangGraphReviewController()\n",
    "\n",
    "print(\"✓ LangGraph ReviewController initialized\")\n",
    "print(f\"  Workflow nodes: parse_sections, analyze_section, next_section, validate_suggestions\")\n",
    "print(f\"  Conditional edges: continue/validate based on section index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RUNNING FULL LANGGRAPH WORKFLOW\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "LANGGRAPH WORKFLOW COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Processing time: 87.53s\n",
      "Session ID: notebook-test-session\n",
      "\n",
      "Metadata:\n",
      "  total_sections: 6\n",
      "  target_venue: NeurIPS\n",
      "  clarity_suggestions: 18\n",
      "  rigor_suggestions: 8\n",
      "  final_suggestions: 6\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RUNNING FULL LANGGRAPH WORKFLOW\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "result = await controller.review(\n",
    "    content=SAMPLE_PAPER,\n",
    "    session_id=\"notebook-test-session\",\n",
    "    target_venue=\"NeurIPS\"\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"LANGGRAPH WORKFLOW COMPLETE\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Processing time: {elapsed_time:.2f}s\")\n",
    "print(f\"Session ID: {result['session_id']}\")\n",
    "print(f\"\\nMetadata:\")\n",
    "for key, value in result['metadata'].items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Analyze Results\n",
    "\n",
    "Analyze the final suggestions from the orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL SUGGESTIONS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Total suggestions: 6\n",
      "\n",
      "By Type:\n",
      "  SuggestionType.CLARITY: 4\n",
      "  SuggestionType.RIGOR: 2\n",
      "\n",
      "By Severity:\n",
      "  SeverityLevel.ERROR: 4\n",
      "  SeverityLevel.WARNING: 2\n",
      "\n",
      "By Section:\n",
      "  Results: 2\n",
      "  Abstract: 2\n",
      "  Introduction: 1\n",
      "  Methods: 1\n"
     ]
    }
   ],
   "source": [
    "suggestions = result['suggestions']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL SUGGESTIONS ANALYSIS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Total suggestions: {len(suggestions)}\\n\")\n",
    "\n",
    "# Group by type\n",
    "by_type = {}\n",
    "for s in suggestions:\n",
    "    stype = s['type']\n",
    "    by_type[stype] = by_type.get(stype, 0) + 1\n",
    "\n",
    "print(\"By Type:\")\n",
    "for stype, count in sorted(by_type.items()):\n",
    "    print(f\"  {stype}: {count}\")\n",
    "\n",
    "# Group by severity\n",
    "by_severity = {}\n",
    "for s in suggestions:\n",
    "    severity = s['severity']\n",
    "    by_severity[severity] = by_severity.get(severity, 0) + 1\n",
    "\n",
    "print(\"\\nBy Severity:\")\n",
    "for severity, count in sorted(by_severity.items()):\n",
    "    print(f\"  {severity}: {count}\")\n",
    "\n",
    "# Group by section\n",
    "by_section = {}\n",
    "for s in suggestions:\n",
    "    section = s['section']\n",
    "    by_section[section] = by_section.get(section, 0) + 1\n",
    "\n",
    "print(\"\\nBy Section:\")\n",
    "for section, count in sorted(by_section.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {section}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Display Top Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOP 10 SUGGESTIONS\n",
      "================================================================================\n",
      "\n",
      "1. [RIGOR] Rigor Issue\n",
      "   Section: Results\n",
      "   Severity: SeverityLevel.ERROR\n",
      "   Lines: 42-55\n",
      "   Description: Lack of statistical significance testing for accuracy results\n",
      "   Suggested Fix: Include statistical significance tests (e.g., t-tests) to validate that the accuracy improvement is not due to random chance.\n",
      "\n",
      "2. [RIGOR] Rigor Issue\n",
      "   Section: Results\n",
      "   Severity: SeverityLevel.ERROR\n",
      "   Lines: 42-55\n",
      "   Description: No mention of statistical analysis for comparison results\n",
      "   Suggested Fix: Include statistical analysis (e.g., confidence intervals) for the performance comparison to support claims of superiority.\n",
      "\n",
      "3. [CLARITY] Clarity Issue\n",
      "   Section: Abstract\n",
      "   Severity: SeverityLevel.ERROR\n",
      "   Lines: 4-8\n",
      "   Description: The phrase 'by a significant margin' is vague and does not provide quantitative context.\n",
      "   Suggested Fix: Provide specific metrics or percentages to quantify the performance improvement over manually designed architectures.\n",
      "\n",
      "4. [CLARITY] Clarity Issue\n",
      "   Section: Abstract\n",
      "   Severity: SeverityLevel.WARNING\n",
      "   Lines: 4-8\n",
      "   Description: The term 'NAS' is not defined, which may confuse readers unfamiliar with the acronym.\n",
      "   Suggested Fix: Define 'NAS' as 'Neural Architecture Search' in the first sentence.\n",
      "\n",
      "5. [CLARITY] Clarity Issue\n",
      "   Section: Introduction\n",
      "   Severity: SeverityLevel.ERROR\n",
      "   Lines: 9-19\n",
      "   Description: The term 'RL' is used without explanation, which may confuse readers unfamiliar with the acronym.\n",
      "   Suggested Fix: Define 'RL' as 'reinforcement learning' when it is first mentioned to ensure clarity for all readers.\n",
      "\n",
      "6. [CLARITY] Clarity Issue\n",
      "   Section: Methods\n",
      "   Severity: SeverityLevel.WARNING\n",
      "   Lines: 20-41\n",
      "   Description: The explanation of the training procedure lacks detail on how the validation accuracy is computed and used.\n",
      "   Suggested Fix: Include a brief description of how validation accuracy is calculated and its role in the training process.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TOP 10 SUGGESTIONS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for i, suggestion in enumerate(suggestions[:10], 1):\n",
    "    print(f\"{i}. [{suggestion['type'].upper()}] {suggestion['title']}\")\n",
    "    print(f\"   Section: {suggestion['section']}\")\n",
    "    print(f\"   Severity: {suggestion['severity']}\")\n",
    "    print(f\"   Lines: {suggestion.get('line_start', 'N/A')}-{suggestion.get('line_end', 'N/A')}\")\n",
    "    print(f\"   Description: {suggestion['description']}\")\n",
    "    if suggestion.get('suggested_fix'):\n",
    "        print(f\"   Suggested Fix: {suggestion['suggested_fix']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: Verify Structured Outputs\n",
    "\n",
    "Verify that all data contracts are properly enforced via Pydantic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STRUCTURED OUTPUT VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "✓ All suggestions have required fields\n",
      "✓ All severity levels are valid\n",
      "✓ All suggestion types are valid\n",
      "\n",
      "✓ Structured output verification complete\n",
      "  All agent responses validated via Pydantic models\n",
      "  Data contracts enforced throughout the workflow\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"STRUCTURED OUTPUT VERIFICATION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Verify all suggestions have required fields\n",
    "required_fields = ['id', 'type', 'severity', 'title', 'description', 'section']\n",
    "all_valid = True\n",
    "\n",
    "for i, suggestion in enumerate(suggestions, 1):\n",
    "    for field in required_fields:\n",
    "        if field not in suggestion:\n",
    "            print(f\"❌ Suggestion {i} missing required field: {field}\")\n",
    "            all_valid = False\n",
    "\n",
    "if all_valid:\n",
    "    print(\"✓ All suggestions have required fields\")\n",
    "\n",
    "# Verify severity values are valid\n",
    "valid_severities = {'info', 'warning', 'error'}\n",
    "invalid_severities = set()\n",
    "\n",
    "for suggestion in suggestions:\n",
    "    if suggestion['severity'] not in valid_severities:\n",
    "        invalid_severities.add(suggestion['severity'])\n",
    "\n",
    "if invalid_severities:\n",
    "    print(f\"❌ Invalid severity levels found: {invalid_severities}\")\n",
    "else:\n",
    "    print(\"✓ All severity levels are valid\")\n",
    "\n",
    "# Verify suggestion types\n",
    "valid_types = {'clarity', 'rigor', 'coherence', 'citation', 'best_practices', 'structure'}\n",
    "invalid_types = set()\n",
    "\n",
    "for suggestion in suggestions:\n",
    "    if suggestion['type'] not in valid_types:\n",
    "        invalid_types.add(suggestion['type'])\n",
    "\n",
    "if invalid_types:\n",
    "    print(f\"❌ Invalid suggestion types found: {invalid_types}\")\n",
    "else:\n",
    "    print(\"✓ All suggestion types are valid\")\n",
    "\n",
    "print(\"\\n✓ Structured output verification complete\")\n",
    "print(\"  All agent responses validated via Pydantic models\")\n",
    "print(\"  Data contracts enforced throughout the workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 7: Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PERFORMANCE METRICS\n",
      "================================================================================\n",
      "\n",
      "Processing Metrics:\n",
      "  Total time: 87.53s\n",
      "  Time per section: 14.59s\n",
      "  Suggestions per second: 0.07\n",
      "\n",
      "Agent Metrics:\n",
      "  Clarity suggestions: 18\n",
      "  Rigor suggestions: 8\n",
      "  Total before orchestrator: 26\n",
      "  Final after orchestrator: 6\n",
      "  Orchestrator filter rate: 76.9%\n",
      "\n",
      "Cost Estimation (gpt-4o-mini):\n",
      "  Estimated tokens: ~1,183\n",
      "  Estimated cost: $0.0005\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PERFORMANCE METRICS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "metadata = result['metadata']\n",
    "total_sections = metadata['total_sections']\n",
    "clarity_count = metadata['clarity_suggestions']\n",
    "rigor_count = metadata['rigor_suggestions']\n",
    "final_count = metadata['final_suggestions']\n",
    "\n",
    "print(f\"Processing Metrics:\")\n",
    "print(f\"  Total time: {elapsed_time:.2f}s\")\n",
    "print(f\"  Time per section: {elapsed_time / total_sections:.2f}s\")\n",
    "print(f\"  Suggestions per second: {len(suggestions) / elapsed_time:.2f}\")\n",
    "\n",
    "print(f\"\\nAgent Metrics:\")\n",
    "print(f\"  Clarity suggestions: {clarity_count}\")\n",
    "print(f\"  Rigor suggestions: {rigor_count}\")\n",
    "print(f\"  Total before orchestrator: {clarity_count + rigor_count}\")\n",
    "print(f\"  Final after orchestrator: {final_count}\")\n",
    "print(f\"  Orchestrator filter rate: {(1 - final_count/(clarity_count + rigor_count))*100:.1f}%\")\n",
    "\n",
    "# Rough token and cost estimation\n",
    "input_tokens = len(SAMPLE_PAPER.split()) * 1.3\n",
    "output_tokens = len(json.dumps(suggestions)) / 4\n",
    "total_tokens = input_tokens + output_tokens\n",
    "\n",
    "# gpt-4o-mini pricing\n",
    "cost_per_1k_input = 0.00015\n",
    "cost_per_1k_output = 0.0006\n",
    "estimated_cost = (input_tokens / 1000 * cost_per_1k_input + \n",
    "                  output_tokens / 1000 * cost_per_1k_output)\n",
    "\n",
    "print(f\"\\nCost Estimation (gpt-4o-mini):\")\n",
    "print(f\"  Estimated tokens: ~{int(total_tokens):,}\")\n",
    "print(f\"  Estimated cost: ${estimated_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 8: Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "output_file = \"notebook_review_results.json\"\n",
    "\n",
    "output_data = {\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"session_id\": result['session_id'],\n",
    "    \"processing_time\": elapsed_time,\n",
    "    \"metadata\": metadata,\n",
    "    \"suggestions\": suggestions,\n",
    "    \"performance\": {\n",
    "        \"time_per_section\": elapsed_time / total_sections,\n",
    "        \"suggestions_per_second\": len(suggestions) / elapsed_time,\n",
    "        \"orchestrator_filter_rate\": (1 - final_count/(clarity_count + rigor_count))*100,\n",
    "        \"estimated_cost\": estimated_cost\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results exported to {output_file}\")\n",
    "print(f\"  File size: {os.path.getsize(output_file)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. ✅ **Section Parsing**: Markdown → Pydantic Section models\n",
    "2. ✅ **Agent Testing**: Individual ClarityAgent and RigorAgent testing\n",
    "3. ✅ **Structured Outputs**: All LLM responses validated via Pydantic\n",
    "4. ✅ **LangGraph Workflow**: Complete StateGraph execution\n",
    "5. ✅ **Orchestrator Validation**: Structured decision making with filtering\n",
    "6. ✅ **Data Contracts**: Type-safe agent communication\n",
    "7. ✅ **Performance Metrics**: Timing, cost, and efficiency analysis\n",
    "\n",
    "The agentic system successfully processes research papers using:\n",
    "- **LangGraph** for workflow orchestration\n",
    "- **Pydantic** for state management and structured outputs\n",
    "- **ReAct pattern** for agent self-reflection\n",
    "- **Structured outputs** for guaranteed data contracts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
