[
  {
    "reference_question": "In our study, we applied a logistic regression model to predict the likelihood of disease presence based on patient demographic data. The model was trained on a dataset comprising 500 samples, achieving an accuracy of 85%. These results highlight the model's potential utility in predictive diagnostics.",
    "reference_context": "Mathematical Writing\u2014Issues of technical writing and the effective presentation of mathematics and computer science. Preparation of theses, papers, books, and 'literate' computer programs.",
    "reference_answer": "Include a discussion of baseline comparisons and statistical testing. Specifically: (1) Compare the logistic regression model's performance with other baseline models (e.g., decision tree, SVM), (2) Conduct statistical significance tests to validate the model's superior performance, (3) Report the p-values from these tests to support the claims of effectiveness. Example: 'Our model achieved 85% accuracy, significantly outperforming the decision tree baseline (80%, p < 0.05, McNemar's test).'",
    "issue_type": "missing_baseline",
    "severity": "warning",
    "domain": "statistics",
    "section_type": "results",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_000"
  },
  {
    "reference_question": "We implemented a novel branch-and-bound algorithm to solve the capacitated vehicle routing problem (CVRP) on a set of benchmark instances. Our approach reduced the total distance traveled by 5% compared to the existing heuristic methods. The algorithm was tested on 10 CVRP instances from the well-known Solomon dataset.",
    "reference_context": "Issues of technical writing and the effective presentation of mathematics and computer science.",
    "reference_answer": "Include a comparison with baseline methods and report statistical significance. Specifically: (1) Clearly state which existing heuristic methods were used as baselines, (2) Run the algorithm over multiple trials to account for variability, (3) Perform statistical analyses such as ANOVA or t-tests to determine the significance of the improvements. Example: 'Our approach reduced the total distance traveled by 5% \u00b1 0.8% (mean \u00b1 std over 5 runs) compared to the Clarke-Wright savings algorithm baseline, with statistical significance confirmed via a paired t-test (p < 0.05).'",
    "issue_type": "missing_baseline",
    "severity": "warning",
    "domain": "operations_research",
    "section_type": "results",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_001"
  },
  {
    "reference_question": "We implemented a novel transformer-based architecture and trained it on the CIFAR-10 dataset. The model achieved a test accuracy of 91.2%, which indicates superior performance over existing models. We utilized a learning rate of 0.001 and a batch size of 128.",
    "reference_context": "We spent the rest of class continuing to examine the homework assignment. In the interest of succinct notes, I have replaced many literal phrases by their generic equivalents. For example, I might have replaced \u2018A > B\u2019 by \u2018\u27e8relation\u27e9\u2019. This time I have divided the comments into two sets: those dealing with what I will call \u201cform\u201d (parentheses, capitalization, fonts, etc.) and those dealing with \u201ccontent\u201d (wording, sentence construction, tense, etc.).",
    "reference_answer": "Report hyperparameter tuning details and rationale. Specifically: (1) Describe the hyperparameter search strategy (e.g., grid search, random search, Bayesian optimization), (2) Provide a range or list of values tested for critical hyperparameters like learning rate and batch size, (3) Explain how the chosen hyperparameters were determined to be optimal. Example: 'We conducted a random search over learning rates [0.0001, 0.001, 0.01] and batch sizes [64, 128, 256], selecting the combination that maximized validation accuracy.'",
    "issue_type": "unreported_hyperparameters",
    "severity": "warning",
    "domain": "machine_learning",
    "section_type": "methods",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_002"
  },
  {
    "reference_question": "We employed a linear programming approach to solve the supply chain optimization problem. The model was tested on a dataset comprising 1000 demand scenarios. We have: the results show a cost reduction of 15% compared to the previous year.",
    "reference_context": "Don\u2019t overdo the use of colons. While the colon in \u2018De\ufb01ne it as follows:\u2019 is \ufb01ne, the one in \u2018We have: \u27e8formula\u27e9\u2019 should be omitted since the formula just completes the sentence.",
    "reference_answer": "Remove the unnecessary colon after 'We have:'. The statement 'the results show a cost reduction of 15% compared to the previous year' should directly follow 'We have' without a colon, as it completes the sentence rather than introducing a separate element.",
    "issue_type": "excessive_colon_use",
    "severity": "info",
    "domain": "operations_research",
    "section_type": "results",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_003"
  },
  {
    "reference_question": "Our proposed algorithm was evaluated on the CIFAR-10 dataset. It achieved a classification accuracy of 85.7%, outperforming existing methods. The results clearly indicate the superiority of our approach.",
    "reference_context": "Each part; and things should be explained twice (formally and informally). These two principles lead naturally to programs made up of modules that begin with text (informal explanation) and finish with Pascal (formal explanation).",
    "reference_answer": "Provide a baseline comparison and detailed explanation. Specifically: (1) Compare the performance against established baseline models (e.g., ResNet, VGG) under the same conditions to support claims of superiority, (2) Describe the experimental setup and parameter configurations used, (3) Include visual aids such as confusion matrices or error bars to convey performance nuances. Example: 'Our algorithm achieved 85.7% accuracy, surpassing the ResNet-18 baseline (82.5%) and VGG-16 (83.2%) using the same training protocol. The experiments were conducted with a learning rate of 0.001 and batch size of 64, across 5 random seeds. Figure 2 shows the confusion matrix, highlighting improved performance on minority classes.'",
    "issue_type": "missing_baseline",
    "severity": "warning",
    "domain": "machine_learning",
    "section_type": "results",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_004"
  },
  {
    "reference_question": "We implemented a branch and bound algorithm to address the mixed-integer linear programming problem. The method was tested on a benchmark set of 50 instances, and our algorithm solved each instance to optimality within an average runtime of 30 seconds.",
    "reference_context": "each part; and things should be explained twice (formally and informally). These two principles lead naturally to programs made up of modules that begin with text (informal explanation) and \ufb01nish with Pascal (formal explanation).",
    "reference_answer": "Include a formal comparison with a baseline method and detailed runtime analysis. Specifically: (1) Compare against a standard solver such as CPLEX or Gurobi, reporting the average runtime and optimality gap for both methods, (2) Provide a breakdown of the runtime components (e.g., node processing time, branching decisions) to understand where improvements occur, (3) Discuss the computational complexity in theoretical terms, explaining the expected performance relative to problem size. Example: 'Our branch and bound algorithm achieved an average runtime of 30 seconds, compared to CPLEX's average of 45 seconds on the same instances, with both achieving optimal solutions.'",
    "issue_type": "missing_baseline",
    "severity": "warning",
    "domain": "operations_research",
    "section_type": "experiments",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_005"
  },
  {
    "reference_question": "In our study, we implemented a new heuristic algorithm to solve the Traveling Salesman Problem (TSP). The algorithm was tested on a single benchmark instance from the TSPLIB library, and the results indicated a substantial reduction in computational time compared to the classic Genetic Algorithm approach.",
    "reference_context": "The referee is conventionally regarded as a sort of \u201cexpert witness,\u201d whose task is to tell",
    "reference_answer": "To improve rigor, include a comparison across multiple benchmark instances and report comprehensive performance metrics. Specifically: (1) Test the algorithm on a diverse set of instances from the TSPLIB library, (2) Provide average computational time and solution quality across all instances, (3) Include a baseline comparison with multiple established algorithms, (4) Conduct statistical analyses, such as ANOVA, to determine the significance of performance differences. Example: 'The new heuristic algorithm was evaluated on 10 different TSPLIB instances, achieving an average computational time of X seconds (\u00b1 Y std) and a mean solution quality of Z% deviation from the optimal, significantly outperforming the Genetic Algorithm (p < 0.05, ANOVA).'",
    "issue_type": "insufficient_sample_size",
    "severity": "warning",
    "domain": "operations_research",
    "section_type": "experiments",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_006"
  },
  {
    "reference_question": "Our proposed model was trained on the CIFAR-10 dataset using a batch size of 64 and a learning rate of 0.001. We observed a test accuracy of 85.2%, which indicates that our model outperforms existing methods. The training was conducted over 50 epochs using stochastic gradient descent (SGD).",
    "reference_context": "The referee is conventionally regarded as a sort of \u201cexpert witness,\u201d whose task is to tell",
    "reference_answer": "Include a comparison with baseline models and report hyperparameters. Specifically: (1) Compare performance against well-established baseline models such as ResNet or VGG, and provide their performance metrics. (2) Detail the hyperparameter settings such as learning rate schedules, weight decay, and data augmentation techniques. Example: 'Our model achieved 85.2% test accuracy, outperforming the ResNet-18 baseline (83.5%) and VGG-16 baseline (82.7%). Hyperparameters were set as follows: initial learning rate 0.001 with cosine annealing, weight decay of 1e-4, and standard CIFAR-10 data augmentation.'",
    "issue_type": "missing_baseline",
    "severity": "warning",
    "domain": "machine_learning",
    "section_type": "experiments",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_007"
  },
  {
    "reference_question": "We conducted a logistic regression analysis to predict customer churn using a dataset of 1,000 customer records. The model included variables such as customer tenure, monthly charges, and contract type. The model achieved an accuracy of 85%, indicating a strong predictive capability.",
    "reference_context": "the proof so that the idea of the proof remains the same, but the proof gets shorter.",
    "reference_answer": "Incorporate a baseline comparison and report on the model's performance relative to it. Specifically, (1) Compare the logistic regression model's performance against a simple baseline, such as a random classifier or a constant prediction model, (2) Provide additional performance metrics beyond accuracy, such as precision, recall, F1-score, and the area under the ROC curve, (3) If applicable, include an explanation of why logistic regression was chosen over other models, and (4) Ensure these additional results and comparisons are clearly reported in the text. For example: 'The logistic regression model achieved an 85% accuracy, outperforming the random baseline (50% accuracy). Additional metrics include a precision of 0.82, recall of 0.86, and an AUC of 0.90, justifying its use over simpler models.'",
    "issue_type": "missing_baseline",
    "severity": "warning",
    "domain": "statistics",
    "section_type": "methods",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_008"
  },
  {
    "reference_question": "We developed a novel reinforcement learning algorithm and applied it to the CartPole-v1 environment. Our algorithm successfully balanced the pole for an average of 500 time steps, showcasing its superiority over existing methods.",
    "reference_context": "The guideline emphasizes the importance of providing a clear and concise proof while maintaining the essence of the original idea. It also highlights the need for clarity in notation and potential pitfalls that could lead to errors if not presented clearly.",
    "reference_answer": "Incorporate baseline comparisons and report hyperparameters. Specifically: (1) Include performance metrics of standard algorithms like DQN or PPO on the same environment for a valid comparison, (2) Detail key hyperparameters (e.g., learning rate, discount factor, exploration strategy) used in the experiments. Example: 'Our algorithm balanced the pole for an average of 500 time steps, outperforming DQN (450 time steps) and PPO (460 time steps). Hyperparameters were tuned via grid search, with a learning rate of 0.001 and a discount factor of 0.99.'",
    "issue_type": "missing_baseline",
    "severity": "warning",
    "domain": "machine_learning",
    "section_type": "experiments",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_009"
  },
  {
    "reference_question": "Our reinforcement learning agent was trained using the PPO algorithm on the OpenAI Gym's CartPole-v1 environment. The model data was processed through a series of transformations, and it achieved a reward score of 200 on average. This suggests that the agent learned to balance the pole effectively.",
    "reference_context": "\u2018. . . data have to . . . \u2019. Now long ago Don was told that \u2018data\u2019 is really plural, but everywhere it is used both as a singular or a plural, even in the reliably conservative (\u2018antediluvian!\u2019 chimed Mary-Claire) New York Times.",
    "reference_answer": "Ensure consistent use of plural for 'data'. Instead of 'The model data was processed', use 'The model data were processed'. This maintains grammatical rigor by treating 'data' as plural. Example: 'The model data were processed through a series of transformations, achieving a reward score of 200 on average.'",
    "issue_type": "grammatical_consistency",
    "severity": "info",
    "domain": "machine_learning",
    "section_type": "methods",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_010"
  },
  {
    "reference_question": "The data indicates that our novel vehicle routing algorithm reduces delivery times by approximately 12% when applied to the VRPTW (Vehicle Routing Problem with Time Windows) benchmark instances. Data was collected from various scenarios, and it clearly demonstrates the superior efficiency of our approach compared to traditional methods.",
    "reference_context": "\u2018. . . data have to . . . \u2019. Now long ago Don was told that \u2018data\u2019 is really plural, but everywhere it is used both as a singular or a plural, even in the reliably conservative (\u2018antediluvian!\u2019 chimed Mary-Claire) New York Times. Don thought it quite right to use it as a singular when referring to data as some kind of collective stuff.",
    "reference_answer": "Ensure that the term 'data' is used consistently as a plural noun. Specifically: (1) Change 'data indicates' to 'data indicate' and 'data was collected' to 'data were collected'. This maintains grammatical consistency and rigor. Example: 'The data indicate that our novel vehicle routing algorithm reduces delivery times... Data were collected from various scenarios, and they clearly demonstrate the superior efficiency of our approach compared to traditional methods.'",
    "issue_type": "grammatical_inconsistency",
    "severity": "info",
    "domain": "operations_research",
    "section_type": "results",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_011"
  },
  {
    "reference_question": "We applied our novel clustering algorithm to the dataset and observed a significant improvement in cluster cohesion. This suggests that our approach is superior to existing methods.",
    "reference_context": "Recognizing the dearth of poetry in CS, Jeff now forbids his students to use it either. 90% of the time it doesn\u2019t matter; the other 10% leaves your readers bewildered. One book presents four ideas in a row and then says 'This leads us to consider . . . '. What leads us to consider?",
    "reference_answer": "Clarify what specifically led to the conclusion about the superiority of the method. Provide detailed comparison metrics with baselines. For instance, specify the metrics used for assessing cluster cohesion (e.g., silhouette score, Davies\u2013Bouldin index) and compare these metrics with those of existing methods. Example: 'Our approach achieved a silhouette score of 0.65 compared to 0.58 by K-Means, indicating improved cluster cohesion.'",
    "issue_type": "missing_baseline",
    "severity": "warning",
    "domain": "machine_learning",
    "section_type": "results",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_012"
  },
  {
    "reference_question": "The optimization algorithm was applied to the supply chain network, resulting in a 15% reduction in overall costs. This confirms the superiority of the proposed method over existing solutions. The algorithm was executed using a custom Python script.",
    "reference_context": "Je\ufb00\u2019s English professor, now a leading poet, told him never to use the non-referential \u2018this\u2019. Recognizing the dearth of poetry in CS, Je\ufb00 now forbids his students to use it either. 90% of the time it doesn\u2019t matter; the other 10% leaves your readers bewildered. One book presents four ideas in a row and then says \u201cThis leads us to consider . . . \u201d. What leads us to consider?",
    "reference_answer": "Clarify the referent for the non-referential 'this'. Specifically: (1) Explicitly state what confirms the superiority of the method. For example, revise to: 'The observed 15% reduction in overall costs, as measured by our cost analysis model, confirms the superiority of the proposed method over existing solutions.' This removes ambiguity and clearly attributes the outcome to the specific results.",
    "issue_type": "vague_referent",
    "severity": "warning",
    "domain": "operations_research",
    "section_type": "results",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_013"
  },
  {
    "reference_question": "In our study, we applied a linear regression model to predict housing prices based on features such as location, size, and age of the property. The model achieved an R-squared value of 0.85, indicating a strong fit to the data. We optimized the model using gradient descent over 100 iterations.",
    "reference_context": "The next exercise has us recast a sentence so as to change emphasis. What are the emphatic positions? The front of the sentence and the back. (\u201cThe middle of a sentence is sort of a slum.\u201d) But she says not to take her word for it; we should write sentences with varying emphasis and \ufb01nd out for ourselves.",
    "reference_answer": "Include a description of baseline models and statistical comparison. Specifically: (1) Report the performance of baseline models such as a simple mean predictor or a decision tree regressor, (2) Conduct statistical tests to compare the predictive accuracy of the linear regression model against these baselines, (3) Use metrics like RMSE or MAE alongside R-squared to provide a more comprehensive evaluation. Example: 'Our linear regression model achieved an R-squared value of 0.85 and an RMSE of 20.5, significantly outperforming the decision tree baseline (R-squared = 0.72, RMSE = 25.3, p < 0.05, paired t-test).'",
    "issue_type": "missing_baseline",
    "severity": "warning",
    "domain": "statistics",
    "section_type": "results",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_014"
  },
  {
    "reference_question": "To evaluate the performance of our novel clustering algorithm, we applied it to the well-known Iris dataset. The algorithm successfully grouped the data into clusters that visually matched the true labels in the dataset. This visual alignment indicates that our approach is effective for clustering tasks.",
    "reference_context": "The guideline to test is one that repeats cyclically, but we would be wrong. Periodic sentences are those whose grammatical and physical ends coincide: We must get adverbials out of the final position.",
    "reference_answer": "Incorporate quantitative evaluation metrics and baseline comparisons. Specifically: (1) Report clustering performance using metrics like Adjusted Rand Index (ARI) or Silhouette Score, (2) Compare against standard clustering algorithms (e.g., k-means, hierarchical clustering) on the same dataset, (3) Present numerical results instead of relying solely on visual inspection. Example: 'Our algorithm achieved an ARI of 0.82, outperforming k-means (ARI: 0.75) and hierarchical clustering (ARI: 0.78) on the Iris dataset.'",
    "issue_type": "missing_baseline",
    "severity": "warning",
    "domain": "machine_learning",
    "section_type": "experiments",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_015"
  },
  {
    "reference_question": "In our study, we developed a novel convolutional neural network (CNN) architecture to classify medical images. The CNN was trained using a stochastic gradient descent optimizer with a learning rate of 0.01. Our model achieves an accuracy of 92% on the validation dataset, indicating its potential in medical image classification tasks.",
    "reference_context": "Examples of this kind of usage seem strange when written down (because we don\u2019t use non-referential whiches in written English), but they sound perfectly normal when heard on the street.",
    "reference_answer": "Include baseline comparisons to establish the relative performance of the proposed model. Specifically: (1) Provide results from established models on the same dataset for direct comparison, (2) Discuss differences in accuracy between the new model and existing baselines, (3) Consider additional metrics like precision, recall, and F1-score to give a comprehensive evaluation of model performance. Example: 'Our model achieves 92% accuracy, surpassing the 88% achieved by ResNet-18 and 89% by VGG-16 on the same validation set. Furthermore, our model's precision, recall, and F1-score are 91%, 93%, and 92%, respectively.'",
    "issue_type": "missing_baseline",
    "severity": "error",
    "domain": "machine_learning",
    "section_type": "results",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_016"
  },
  {
    "reference_question": "In our study, we applied a mixed-integer linear programming model to optimize the supply chain network for a large retail company. The model was tested on a dataset consisting of 500 warehouses and 2000 retail outlets, illustrating its scalability and effectiveness. The results indicated a 12% reduction in total costs compared to the current operational strategy.",
    "reference_context": "are shorter than our written ones. People do use which when they talk, but they use non-referential whiches to introduce new thoughts that are tacked on to old thoughts. Examples of this kind of usage seem strange when written down (because we don\u2019t use non-referential whiches in written English), but they sound perfectly normal when heard on the street. Here is one: I went sailing this weekend; which tells you why my nose is pink. Fowler realized that written English would sound more like speech if the choice of relative pronoun was uniquely determined by whether or not the clause it introduced was restrictive or non-restrictive.",
    "reference_answer": "Include details on baseline comparisons and statistical tests. Specifically: (1) Define a clear baseline model or strategy for comparison, (2) Conduct and report statistical tests to confirm the significance of the 12% cost reduction, such as a paired t-test or ANOVA if multiple baselines are used, (3) Include confidence intervals for the cost reduction metric. Example: 'The model achieved a 12% \u00b1 1% reduction in total costs compared to the baseline strategy (p < 0.05, paired t-test), demonstrating significant improvements.'",
    "issue_type": "no_statistical_test",
    "severity": "error",
    "domain": "operations_research",
    "section_type": "results",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_017"
  },
  {
    "reference_question": "In our study, we implemented a new scheduling algorithm for optimizing factory operations, called OptiSched. We tested the algorithm on a set of standard benchmark problems, achieving a reduction in total processing time by up to 20%. Then, the algorithm was applied to a real-world dataset, showing promising results.",
    "reference_context": "We saw a sentence that contained four or \ufb01ve occurrences of the word \u2018then\u2019\u2014surely a tri\ufb02e excessive? Someone remarked that the sentence was probably an anglicised version of a line of computer code.",
    "reference_answer": "Clarify the sequence and context of the experiments. Specifically: (1) Clearly separate the discussion of benchmark tests and real-world applications, (2) Avoid overusing 'then', which can suggest an unclear or overly sequential narrative. For example: 'Our new scheduling algorithm, OptiSched, was first evaluated on standard benchmark problems, achieving a reduction in total processing time by up to 20%. Subsequently, we applied the algorithm to a real-world dataset, where it also showed promising results.'",
    "issue_type": "unclear_sequence",
    "severity": "info",
    "domain": "operations_research",
    "section_type": "experiments",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_018"
  },
  {
    "reference_question": "In our study, we applied a logistic regression model to predict the probability of disease occurrence based on several predictors including age, BMI, and smoking status. The model demonstrated that age was a significant predictor, which was then used to stratify the population into risk categories. Moreover, by saving the computation for the place where it is really needed, we optimized the model's performance.",
    "reference_context": "Another line that caught Rosalie\u2019s eye was: \u201c. . . saving the computation for the place where it is really needed.\u201d The word \u2018really\u2019 was used again the same paragraph. She thought this sounded altogether too vague for a piece of technical writing: Is the computation needed or not? What is this \u201creally needed\u201d?",
    "reference_answer": "Clarify what 'saving the computation for the place where it is really needed' entails. Specifically: (1) Define which computations are being saved and why they are necessary only in certain contexts, (2) Describe how this optimization impacts model performance quantitatively, (3) Provide a rationale for the choice of computational optimization and its relevance to the analysis. Example: 'We applied computational optimization by pre-computing the risk scores only for high-risk groups, reducing unnecessary calculations for low-risk categories, which decreased computation time by 20% without sacrificing accuracy.'",
    "issue_type": "vague_language",
    "severity": "warning",
    "domain": "statistics",
    "section_type": "methods",
    "source": "guideline_generated",
    "guideline_source_file": "/Users/arnabbhattacharya/Desktop/AIE8-certification-challenge/backend/eval/golden_dataset/../../app/resources/rigor_docs/knuth_mathematical_writing.pdf",
    "seed_id": "rigor_seed_019"
  }
]